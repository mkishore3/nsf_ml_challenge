Day 0: Getting Ready to Dive In
Even before the hackathon officially began, we got together to brainstorm and choose the dataset that excited us the most. At first, the butterfly anomaly detection challenge caught our eye. To get a head start, we began downloading and previewing the data. However, it didn’t take long for us to realize that downloading those high-resolution butterfly images was painfully slow, and time wasn’t on our side. So, after some discussion, we decided to switch gears and tackle the sea-level anomaly detection challenge instead.
Day 1: Figuring Things Out
The first day was all about diving into the data and figuring out what we were working with. We spent a lot of time understanding how to load the data into the Jupyter environment and exploring the dataset’s features. One major hurdle was understanding the SLA (Sea Level Anomaly) feature—by far the most important variable for our project. It wasn’t intuitive at first, but we eventually cracked it.
Initially, we thought we’d use the SLA data with a Random Forest Classifier, but mapping the multidimensional arrays to something the model could understand turned out to be much harder than expected. After getting advice from mentors and volunteers, we realized the dataset’s labeling was pretty unconventional. This led us to switch approaches and move towards using a CNN (Convolutional Neural Network). Our idea was to generate daily heat maps for 12 cities as input for the CNN, which would output a binary result—0 or 1—to indicate whether a given day had an anomalous sea level event.
To test the feasibility of generating thousands of heat maps, we created 50 sample plots by the end of Day 1. This gave us a better sense of the time and resources needed to scale up.
Day 2: Making It Happen
With a clear game plan, we started Day 2 determined to bring our CNN approach to life. One of the biggest challenges was generating all the plots. High-resolution heat maps were taking too long to create, so we decided to downsize the images to speed things up. We also broke the data into smaller chunks of 1,500 plots at a time to avoid overloading our systems. These optimizations worked, and by the end of the day, we had generated over 4,000 plots—more than enough to train a robust model.
Next, we built our CNN model using TensorFlow. The model had three hidden layers with ReLU activation functions, and the final output layer used sigmoid activation to produce binary predictions. Once the training was complete, we used the model’s predictions to generate a CSV submission file. Seeing our results come together was a rewarding moment after all the effort we’d put in.
Reflecting on the Journey
The hackathon was an intense but rewarding experience. It wasn’t just about building a solution—it was about staying flexible, solving problems as they came up, and working together as a team. From switching datasets on Day 0 to scaling up our plot generation on Day 2, every step taught us something new. While the challenges were real, so was our determination to push through and make it happen. It was amazing to see what we could accomplish in such a short amount of time, and we’re proud of how far we came.